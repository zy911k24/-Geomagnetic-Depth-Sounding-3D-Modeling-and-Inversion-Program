{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3zy37iP32gQ"
      },
      "source": [
        "### Disclaimer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjTED3cZ32gR"
      },
      "source": [
        "The following notebook was compiled for the course 'Geostatistics' at Ghent University (lecturer-in-charge: Prof. Dr. Ellen Van De Vijver; teaching assistant: Pablo De Weerdt). It consists primarily of notebook snippets created by Michael Pyrcz. The code and markdown (text) snippets were edited specifically for this course, using the 'Jura data set' (Goovaerts, 1997) as example in the practical classes. Some new code snippets are also included to cover topics which were not found in the Geostastpy package demo books.<br>\n",
        "\n",
        "This notebook is for educational purposes.<br>\n",
        "\n",
        "Guidelines for getting started were adapted from the 'Environmental Soil Sensing' course at Ghent University (lecturer-in-charge: Prof. Dr. Philippe De Smedt).<br>\n",
        "\n",
        "The Jura data set was taken from: Goovaerts P., 1997. Geostatistics for Natural Resources Evaluation. Oxford University Press."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J07lzez232gR"
      },
      "source": [
        "# Geostatistics: Introduction to geostatistical data analysis with Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyDn1Msd32gR"
      },
      "source": [
        "### Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxZMqsCM32gR"
      },
      "source": [
        "\n",
        "---\n",
        "This introductory notebook will help you perform an explorative analysis of spatial data, and help get you acquainted with basic data operations in Python. It contains code that you can readily use in the Geostatistics practical sessions.<br>\n",
        "\n",
        "**First of all, make sure to save a copy of this notebook**. In Google Colab, you can do this by navigating to the top menu, click 'File', 'Save a copy in Drive'. Make sure to give an appropriate name to your copy, in this context, for example, the name of the original notebook can be followed by your name or initials.<br>\n",
        "\n",
        "This [jupyter notebook](https://docs.jupyter.org/en/latest/) is made up of text cells (markdown) like this one, and code cells (with [Python](https://docs.python.org/3/tutorial/) code), such as the two cells below these. Note that in code cells, there also is text (comments), which is preceded by a hash (`# This is a comment`) or placed between two or three quatation marks (`'''This is a comment'''`).\n",
        "\n",
        "In these first cell you will install and import specific packages (ready-made sets of code) that you will use to perform analyses in this notebook. The install script has functionality in Colab, Jupyter as well as in standard integrated development environmnents (IDEs).\n",
        "\n",
        "If you are unfamiliar with Python code and Jupyter notebooks, you find some introductory notebooks in the [DS-python-geospatial repository](https://github.com/jorisvandenbossche/DS-python-geospatial/tree/main/notebooks), developed by Joris Van den Bossche. However, since you can run this notebook with Google Colab, all steps are automated and you should have a smooth experience in using all this! Extensive documentation is provided in these code cells to help you along the way.\n",
        "\n",
        "---\n",
        "\n",
        "There is a lot of Python code in this notebook. While you are free to modify the code as you want, in cases where you can perform specific operations or data analyses, the part of the code where you can modify variables, or write functions, always appears above a commented line of asterisks, like this: `# ******* `. The part below the asterisk line is where the rest of the code that is required to perform the operation is written, but to complete the practiical exercise you are not required to change anything.\n",
        "\n",
        "If no asterisk line is present in a code cell, this means you can simply run the code cell without changing anything to get the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRKJ9jl-32gS"
      },
      "outputs": [],
      "source": [
        "# Import required packages for setup\n",
        "# -------------------------------------------- #\n",
        "\n",
        "import sys\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50R5X5Sd32gS"
      },
      "outputs": [],
      "source": [
        "#  Clone the repository and add it to the path\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "\n",
        "    repo_path = '/content/E_I002454_Geostatistics'\n",
        "    if not os.path.exists(repo_path):\n",
        "        !git clone https://github.com/SENSE-UGent/E_I002454_Geostatistics.git\n",
        "    if repo_path not in sys.path:\n",
        "        sys.path.append(repo_path) #Default location in Google Colab after cloning\n",
        "\n",
        "else:\n",
        "    # if you are not using Google Colab, change the path to the location of the repository\n",
        "\n",
        "    repo_path = r'c:/Users/pdweerdt/Documents/Repos/E_I002454_Geostatistics' # Change this to the location of the repository on your machine\n",
        "    if repo_path not in sys.path:\n",
        "        sys.path.append(repo_path)\n",
        "\n",
        "# Import the setup function\n",
        "from Utils.setup import check_and_install_packages\n",
        "\n",
        "# Read the requirements.txt file\n",
        "\n",
        "requirements_path = repo_path + '/Utils/requirements.txt'\n",
        "\n",
        "with open(requirements_path) as f:\n",
        "    required_packages = f.read().splitlines()\n",
        "\n",
        "# Check and install packages\n",
        "check_and_install_packages(required_packages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQe-0vgj32gS"
      },
      "source": [
        "#### Load Required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xUx8_0832gS"
      },
      "outputs": [],
      "source": [
        "import geostatspy.GSLIB as GSLIB                              # GSLIB utilities, visualization and wrapper\n",
        "# import geostatspy.geostats as geostats                      # GSLIB methods convert to Python\n",
        "import geostatspy\n",
        "print('GeostatsPy version: ' + str(geostatspy.__version__))   # these notebooks were tested with GeostatsPy version: 0.0.72"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmwmfELk32gS"
      },
      "outputs": [],
      "source": [
        "from Utils.func import add_grid, add_grid2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCiGv0VN32gS"
      },
      "source": [
        "We will also need some standard packages. These should have been installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYiavFqg32gT"
      },
      "outputs": [],
      "source": [
        "import os                                                     # set working directory, run executables\n",
        "\n",
        "from tqdm import tqdm                                         # suppress the status bar\n",
        "from functools import partialmethod\n",
        "\n",
        "tqdm.__init__ = partialmethod(tqdm.__init__, disable=True)\n",
        "\n",
        "import numpy as np                                            # ndarrays for gridded data\n",
        "\n",
        "import pandas as pd                                           # DataFrames for tabular data\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt                               # for plotting\n",
        "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks\n",
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.ticker as mtick\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "from scipy import stats                                       # summary statistics\n",
        "from scipy.stats import pearsonr                              # Pearson product moment correlation\n",
        "from scipy.stats import spearmanr                             # spearman rank correlation\n",
        "\n",
        "import seaborn as sns                                         # advanced plotting\n",
        "\n",
        "plt.rc('axes', axisbelow=True)                                # plot all grids below the plot elements\n",
        "\n",
        "ignore_warnings = True                                        # ignore warnings?\n",
        "if ignore_warnings == True:\n",
        "    import warnings\n",
        "    warnings.filterwarnings('ignore')\n",
        "\n",
        "from IPython.utils import io                                  # mute output from simulation\n",
        "\n",
        "seed = 42                                                     # random number seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMOPKNBl32gT"
      },
      "source": [
        "### Set the Working Directory\n",
        "\n",
        "Do this to simplify subsequent reads and writes (avoid including the full address each time)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la2Y3H0232gT"
      },
      "source": [
        "##### For use in Google Colab\n",
        "\n",
        "Run the following cell if you automatically want to get the data from the repository and store it on your Google Colab drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypc9Clno32gT"
      },
      "outputs": [],
      "source": [
        "# get the current directory and store it as a variable\n",
        "\n",
        "cd = os.getcwd()\n",
        "print('Current Working Directory is ', cd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRtyyFUi32gT"
      },
      "source": [
        "##### For local use\n",
        "\n",
        "Only run the following cell if you have the data locally stored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTmYPIhA32gT"
      },
      "outputs": [],
      "source": [
        "# set the working directory, place an r in front to address special characters\n",
        "os.chdir(r'c:\\Users\\pdweerdt\\Documents\\Repos')\n",
        "\n",
        "# get the current directory and store it as a variable\n",
        "\n",
        "cd = os.getcwd()\n",
        "print('Current Working Directory is ', cd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8aJG8bw32gT"
      },
      "source": [
        "#### Loading Tabular Data\n",
        "\n",
        "Here's the command to load our data file into a Pandas' DataFrame object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs3GlQNV32gT"
      },
      "source": [
        "We will make use of a function to convert the GSLIB Geo-EAS file to a pandas dataframe for use with Python methods. You can check the GSLIB code file with all available functions including data type conversions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGfHDzrr32gU"
      },
      "outputs": [],
      "source": [
        "# Here you can adjust the relative Path to the data folder\n",
        "\n",
        "data_path = cd + '/E_I002454_Geostatistics/Hard_data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQM2_cfP32gU"
      },
      "outputs": [],
      "source": [
        "file_name = '//prediction.dat'\n",
        "\n",
        "df = GSLIB.GSLIB2Dataframe(data_path + file_name) # read the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYYJaHWW32gU"
      },
      "source": [
        "#### Visualizing the DataFrame\n",
        "\n",
        "Visualizing the DataFrame is always a good idea as a first order check.\n",
        "\n",
        "We can preview the DataFrame by printing a slice or by utilizing the 'head' DataFrame member function (with a nice and clean format, see below). With the slice we could look at any subset of the data table and with the head command, add parameter 'n=13' to see the first 13 rows of the dataset.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZrIcJ4r32gU"
      },
      "outputs": [],
      "source": [
        "print(df.iloc[0:5,:])                                         # display first 4 samples in the table as a preview\n",
        "df.head(n=13)                                                 # we could also use this command for a table preview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUTgEpqI32gU"
      },
      "source": [
        "Please note that in Python we start counting from 0 instead of from 1 !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNVgU4DV32gU"
      },
      "source": [
        "#### Manipulate the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gi1KjkTR32gU"
      },
      "outputs": [],
      "source": [
        "# example of filtering data\n",
        "\n",
        "df_filtered = df[df['Co'] <= 4]\n",
        "df_filtered.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwbyX88432gU"
      },
      "source": [
        "### Univariate Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkVn_j8132gV"
      },
      "source": [
        "This chapter is a tutorial for / demonstration of **Univariate Distributions** in Python with GeostatsPy including:\n",
        "\n",
        "* Histograms\n",
        "* Probability Density Functions\n",
        "* Cumulative Distribution Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz90Lp5p32gV"
      },
      "source": [
        "#### Summary Univariate Statistics for Tabular Data\n",
        "\n",
        "Data were recorded at 359 locations scattered in space and the concentration of 7 heavy metals in the soil was measured at each location: cadmium, cobalt, chromium, copper, nickel, lead and zinc. The prediction dataset contains 259 of those locations.\n",
        "\n",
        "There are a lot of efficient methods to calculate summary statistics from tabular data in DataFrames. The describe command provides count, mean, minimum, maximum, and quartiles all in a nice data table. We use transpose just to flip the table so that features are on the rows and the statistics are on the columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3krZnKJp32gV"
      },
      "outputs": [],
      "source": [
        "df.describe()     # summary statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtjhzeT132gV"
      },
      "outputs": [],
      "source": [
        "df.describe().transpose() # flipped the table to have the statistics as rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHfSD3ai32gV"
      },
      "source": [
        "#### Feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVp-2kOP32gV"
      },
      "source": [
        "We can define a feature of interest and calculate some more statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ky3CmmSW32gV"
      },
      "outputs": [],
      "source": [
        "feature = 'Cd'  # feature of interest\n",
        "\n",
        "unit = 'ppm'  # unit of measurement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N180VuAX32gV"
      },
      "source": [
        "We can also use a wide variety of statistical summaries built into NumPy's ndarrays.  When we use the command:\n",
        "```p\n",
        "df[feature]                       # returns an Pandas series\n",
        "df[feature].values                # returns an ndarray\n",
        "```\n",
        "Panda's DataFrame returns all the data as a series and if we add 'values' it returns a NumPy ndarray and we have access to a lot of NumPy methods. I also like to use the round function to round the answer to a limited number of digits for accurate reporting of precision and ease of reading.\n",
        "\n",
        "For example, now we could use commands. like this one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aq2B1xXz32gV"
      },
      "outputs": [],
      "source": [
        "min = round((df[feature].values).min(),2)                    # calculate the minimum\n",
        "max = round((df[feature].values).max(),2)                    # calculate the maximum\n",
        "mean = round((df[feature].values).mean(),2)                  # calculate the mean\n",
        "stdev = round((df[feature].values).std(),2)                  # calculate the standard deviation\n",
        "n = df[feature].values.size                                  # calculate the number of data\n",
        "\n",
        "\n",
        "print('The minimum is ' + str(min) + ' ' + str(unit)+ '.') # print univariate statistics\n",
        "print('The maximum is ' + str(max) + ' ' + str(unit)+ '.')\n",
        "print('The mean is ' + str(mean) + ' ' + str(unit)+ '.')\n",
        "print('The standard deviation is ' + str(stdev) + ' ' + str(unit) + '.')\n",
        "print('The number of data is ' + str(n) + '.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCG1tDhq32gV"
      },
      "source": [
        "Note that SciPy stats functions provide a handy summary statistics function. The output is a 'list' of values (actually it is a SciPy.DescribeResult object). This list includes skewness and kurtosis. One can extract any one of them to use in a workflow as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmtsU8EH32gV"
      },
      "outputs": [],
      "source": [
        "print(stats.describe(df[feature].values))                  # summary statistics\n",
        "feature_stats = stats.describe(df[feature].values)             # store as an array\n",
        "print(str(feature) + ' kurtosis is ' + str(round(feature_stats[5],2)))   # extract a statistic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53bSZYkk32gV"
      },
      "source": [
        "Now write some code in the cell below to calculate some statistics by hand using the formulas as seen in theory class. Compare the outcome with the previous cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DooO0CHb32gW"
      },
      "outputs": [],
      "source": [
        "n = # number of data\n",
        "mean =\n",
        "var = # variance\n",
        "stdev = # standard deviation\n",
        "\n",
        "kurt = # kurtosis\n",
        "skew = # skewness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYE14xXh32gW"
      },
      "source": [
        "#### Histograms\n",
        "\n",
        "Let's display some histograms. I reimplemented the hist function from GSLIB. Preview the parameters by typing the command without parameters.\n",
        "* also to learn about function parameters the alt-tab key combination with cursor in the function parentheses is often available to access the \"docstrings\" or check the Python package docs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnDaEEKW32gW"
      },
      "outputs": [],
      "source": [
        "GSLIB.hist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t538AoKD32gW"
      },
      "source": [
        "Let's make a histogram for the feature.\n",
        "\n",
        "Check out the function arguments first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uf8HkaEM32gW"
      },
      "outputs": [],
      "source": [
        "GSLIB.hist_st(df[feature].values,\n",
        "              xmin = min, xmax = max,   # minimum and maximum feature values\n",
        "              log=False,cumul = False,bins=10,weights = None,\n",
        "           xlabel= str(feature) + ' (' + str(unit) + ')', title=str(feature) + ' Data')\n",
        "add_grid()\n",
        "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.1, hspace=0.2); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiN_Mer032gW"
      },
      "source": [
        "What's going on here? Describe.\n",
        "\n",
        "##### Histogram Bins, Number of Bins and Bin Size\n",
        "\n",
        "Let's explore with a few bins sizes to check the impact on the histogram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVbgDFAN32gW"
      },
      "outputs": [],
      "source": [
        "nbin_list = [3,20,100]                                             # number of bins for each histogram\n",
        "\n",
        "for i,nbin in enumerate(nbin_list):\n",
        "    plt.subplot(3,1,i+1)\n",
        "    GSLIB.hist_st(df[feature].values, xmin = min, xmax = max, log=False,cumul = False, bins=nbin, weights = None,\n",
        "                  xlabel= str(feature) + ' (' + str(unit) + ')', title='Histogram with ' + str(nbin) + ' Bins'); add_grid()\n",
        "\n",
        "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=3.1, wspace=0.2, hspace=0.2); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9h46-WH32gW"
      },
      "source": [
        "See what happens when we use:\n",
        "\n",
        "* **too large bins / too few bins** - often smooth out, removes information\n",
        "* **too small bins / too many bins** - often too noisy, obscures information  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP0wEx_C32gW"
      },
      "source": [
        "#### Cumulative Distribution Functions\n",
        "\n",
        "This method in GeostatsPy makes a cumulative histogram.\n",
        "\n",
        "* you could increase or decrease the number of bins, $> n$ is data resolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahAa-3V232gW"
      },
      "outputs": [],
      "source": [
        "GSLIB.hist_st(df[feature].values, min, max, log=False, cumul = True, bins=1000, weights = None, # CDF with GeostatsPy\n",
        "           xlabel=(str(feature) + ' (' + str(unit) + ')'), title='Cumulative Distribution Function with Cumulative Histogram'); add_grid()\n",
        "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.1, hspace=0.2); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grcZXa5n32gW"
      },
      "source": [
        "#### Distribution Comparison\n",
        "Here we compare the distribution of one variable from two different datasets (e.g. from different sampling campagins, after a data split, ...) or two variables (e.g. to evaluate normality)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkC_zoi532gX"
      },
      "source": [
        "This chapter is a tutorial for / demonstration of **QQ-Plots and PP-Plots**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loaP5WId32gX"
      },
      "source": [
        "##### Evaluation of normality\n",
        "\n",
        "Create a univariate standard normal distribution and create a Gaussian (normal) distribution with same mean and std as our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0E2vwWJ32gX"
      },
      "outputs": [],
      "source": [
        "X_standard_normal = np.random.standard_normal(size=n)\n",
        "\n",
        "X_feature_normal = np.random.normal(loc=mean,scale=stdev,size=100000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjiUypfk32gX"
      },
      "source": [
        "##### Calculate the QQ-plot\n",
        "\n",
        "Calculate and match percentiles from both data distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbjINfmS32gX"
      },
      "outputs": [],
      "source": [
        "nq = 100                                                     # the number of points (equal cumulative probability) sampled for the QQ-plot\n",
        "xmin=min; xmax=max                                          # the range values for the plot axes\n",
        "\n",
        "cumul_prob = np.linspace(1,99,nq)                             # cumulative probability array\n",
        "X_standard_normal_percentiles = np.percentile(X_standard_normal, cumul_prob)                 # calculate all percentiles for plotting\n",
        "X_feature_normal_percentiles = np.percentile(X_feature_normal, cumul_prob)\n",
        "X_feature_percentiles = np.percentile(df[feature].values, cumul_prob)                      # calculate feature percentiles for plotting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1chdnZAz32gX"
      },
      "source": [
        "##### Make the Q-Q Plot Visualization\n",
        "\n",
        "Let's look at the data histograms, cumulative distribution functions and QQ-plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97Qap-xO32gX"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "spec = fig.add_gridspec(2, 3)\n",
        "\n",
        "ax0 = fig.add_subplot(spec[:, 1:])\n",
        "plt.scatter(X_feature_percentiles, X_feature_normal_percentiles, color='darkorange', edgecolor='black', s=10, label='Q-Q plot')\n",
        "plt.plot([0, 3.5], [0, 3.5], ls='--', color='red') # 1:1 line\n",
        "plt.grid(); plt.xlim([xmin, xmax]); plt.ylim([xmin, xmax]); plt.xlabel(str(feature) + ' (' + str(unit) + ')'); plt.ylabel('$' + str(feature) + '_{\\\\mathrm{normal}}$ ' + ' (' + str(unit) + ')')\n",
        "plt.title('Q-Q Plot'); plt.legend(loc='lower right')\n",
        "add_grid2(ax0)\n",
        "\n",
        "ax10 = fig.add_subplot(spec[0, 0])\n",
        "plt.hist(df[feature].values, bins=np.linspace(xmin, xmax, 30), color='red', alpha=0.5, edgecolor='black', label='$' + str(feature) + '$', density=True)\n",
        "plt.hist(X_feature_normal, bins=np.linspace(xmin, xmax, 30), color='yellow', alpha=0.5, edgecolor='black', label='$' + str(feature) + '_{\\\\mathrm{normal}}$', density=True)\n",
        "plt.grid(); plt.xlim([xmin, xmax]);\n",
        "# plt.ylim([0, 15]);\n",
        "plt.xlabel(str(feature) + ' (' + str(unit) + ')'); plt.ylabel('Density')\n",
        "plt.title('Histograms'); plt.legend(loc='upper right')\n",
        "add_grid2(ax10)\n",
        "\n",
        "ax11 = fig.add_subplot(spec[1, 0])\n",
        "plt.scatter(np.sort(df[feature].values), np.linspace(0, 1, len(df[feature].values)), color='red', edgecolor='black', s=10, label='$' + str(feature) + '$')\n",
        "plt.scatter(np.sort(X_feature_normal), np.linspace(0, 1, len(X_feature_normal)), color='yellow', edgecolor='black', s=10, label='$' + str(feature) + '_{\\\\mathrm{normal}}$')\n",
        "plt.grid(); plt.xlim([xmin, xmax]); plt.ylim([0, 1]); plt.xlabel(str(feature) + ' (' + str(unit) + ')'); plt.title('CDFs'); plt.legend(loc='lower right')\n",
        "add_grid2(ax11)\n",
        "\n",
        "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.4, wspace=0.3, hspace=0.3); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4SHioT132gX"
      },
      "source": [
        "##### Distribution comparison of one variable from two datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBZ7vqQP32gX"
      },
      "source": [
        "##### Load in a second dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O-qD_u-32gX"
      },
      "outputs": [],
      "source": [
        "file_name = '//validation' #without the extension\n",
        "\n",
        "df_2 = GSLIB.GSLIB2Dataframe(data_path +  file_name + '.dat')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUrk17Nh32gX"
      },
      "source": [
        "##### Calculate the QQ-plot\n",
        "\n",
        "Calculate and match percentiles from both data distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbEjDCxN32gX"
      },
      "outputs": [],
      "source": [
        "nq = 100                                                      # the number of points (equal cumulative probability) sampled for the QQ-plot\n",
        "xmin=min; xmax=max                                          # the range values for the plot axes\n",
        "\n",
        "cumul_prob = np.linspace(1,99,nq)                             # cumulative probability array\n",
        "\n",
        "X2_feature_percentiles = np.percentile(df_2[feature].values, cumul_prob)                      # calculate feature percentiles for plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_ykcHuJ32gX"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "spec = fig.add_gridspec(2, 3)\n",
        "\n",
        "ax0 = fig.add_subplot(spec[:, 1:])\n",
        "plt.scatter(X_feature_percentiles, X2_feature_percentiles, color='darkorange', edgecolor='black', s=10, label='Q-Q plot')\n",
        "plt.plot([0, 3.5], [0, 3.5], ls='--', color='red') # 1:1 line\n",
        "plt.grid(); plt.xlim([xmin, xmax]); plt.ylim([xmin, xmax]); plt.xlabel(str(feature) + ' (' + str(unit) + ')'); plt.ylabel('$' + str(feature) + '_{\\\\mathrm{2}}$ ' + ' (' + str(unit) + ')')\n",
        "plt.title('Q-Q Plot'); plt.legend(loc='lower right')\n",
        "add_grid2(ax0)\n",
        "\n",
        "ax10 = fig.add_subplot(spec[0, 0])\n",
        "plt.hist(df[feature].values, bins=np.linspace(xmin, xmax, 30), color='red', alpha=0.5, edgecolor='black', label='$' + str(feature) + '$', density=True)\n",
        "plt.hist(df_2[feature].values, bins=np.linspace(xmin, xmax, 30), color='yellow', alpha=0.5, edgecolor='black', label='$' + str(feature) + '_{\\\\mathrm{2}}$', density=True)\n",
        "plt.grid(); plt.xlim([xmin, xmax]);\n",
        "# plt.ylim([0, 15]);\n",
        "plt.xlabel(str(feature) + ' (' + str(unit) + ')'); plt.ylabel('Density')\n",
        "plt.title('Histograms'); plt.legend(loc='upper right')\n",
        "add_grid2(ax10)\n",
        "\n",
        "ax11 = fig.add_subplot(spec[1, 0])\n",
        "plt.scatter(np.sort(df[feature].values), np.linspace(0, 1, len(df[feature].values)), color='red', edgecolor='black', s=10, label='$' + str(feature) + '$')\n",
        "plt.scatter(np.sort(df_2[feature].values), np.linspace(0, 1, len(df_2[feature].values)), color='yellow', edgecolor='black', s=10, label='$' + str(feature) + '_{\\\\mathrm{2}}$')\n",
        "plt.grid(); plt.xlim([xmin, xmax]); plt.ylim([0, 1]); plt.xlabel(str(feature) + ' (' + str(unit) + ')'); plt.title('CDFs'); plt.legend(loc='lower right')\n",
        "add_grid2(ax11)\n",
        "\n",
        "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.4, wspace=0.3, hspace=0.3); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0juM0a232gX"
      },
      "source": [
        "### Plotting Spatial Data\n",
        "\n",
        "Michael J. Pyrcz, Professor, The University of Texas at Austin\n",
        "\n",
        "[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [Applied Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)\n",
        "\n",
        "Chapter of e-book \"Applied Geostatistics in Python: a Hands-on Guide with GeostatsPy\".\n",
        "\n",
        "Cite as: Pyrcz, M.J., 2024, Applied Geostatistics in Python: a Hands-on Guide with GeostatsPy, https://geostatsguy.github.io/GeostatsPyDemos_Book.\n",
        "\n",
        "By Michael J. Pyrcz <br />\n",
        "&copy; Copyright 2024."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBW1x8Ee32gY"
      },
      "source": [
        "This is a tutorial for / demonstration of **Visualizing Spatial Data** with GeostatsPy, including,\n",
        "\n",
        "* **location maps** for plotting tabular spatial data, data points in space with one or more features\n",
        "* **pixel plots** for plotting gridded, exhaustive spatial data and models with one or more features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oobom3sM32gY"
      },
      "source": [
        "####  Colorbar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xJv4dOx32gY"
      },
      "source": [
        "Check the\n",
        "* [Matplotlib colormaps](https://matplotlib.org/stable/users/explain/colors/colormaps.html) for plotting with matplotlib\n",
        "* [seaborn color palettes](https://seaborn.pydata.org/generated/seaborn.color_palette.html) for plotting with seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVVfbGBs32gY"
      },
      "outputs": [],
      "source": [
        "cmap = plt.cm.inferno                                         # color map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLUefmYP32gY"
      },
      "source": [
        "#### Summary Statistics\n",
        "\n",
        "Let's look at summary statistics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmYXB70a32gY"
      },
      "outputs": [],
      "source": [
        "df.describe().transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vKWkwwl32gY"
      },
      "source": [
        "#### Specify the Area of Interest / Grid and Feature Limits\n",
        "\n",
        "Let's specify a reasonable extents for our grid and features:\n",
        "\n",
        "* we do this so we have consistent plots for comparison.\n",
        "\n",
        "* we design a grid that balances detail and computation time. Note kriging computation complexity scales\n",
        "\n",
        "* so if we half the cell size we have 4 times more grid cells in 2D, 4 times the runtime\n",
        "\n",
        "We could use commands like this one to find the minimum value of a feature:\n",
        "```python\n",
        "df[feature].min()\n",
        "```\n",
        "* But, it is natural to set the ranges manually. e.g. do you want your color bar to go from 0.05887 to 0.24230 exactly?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6laXVnn32gY"
      },
      "outputs": [],
      "source": [
        "xmin = 0; xmax = np.ceil(df.Xloc.max())                                   # range of x values\n",
        "ymin = 0; ymax = np.ceil(df.Yloc.max())                                   # range of y values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3r-vNOu32gY"
      },
      "source": [
        "#### Visualizing Tabular Data with Location Maps\n",
        "\n",
        "Let's try out locmap. This is a reimplementation of GSLIB's locmap program that uses matplotlib. I hope you find it simpler than matplotlib, if you want to get more advanced and build custom plots lock at the source. If you improve it, send me the new code. Any help is appreciated. To see the parameters, just type the command name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vkns1Ix432gY"
      },
      "outputs": [],
      "source": [
        "GSLIB.locmap                                                # GeostatsPy's location map function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osLF9Ypi32gY"
      },
      "source": [
        "Let's add the plotting parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZM1x4Z932gY"
      },
      "outputs": [],
      "source": [
        "GSLIB.locmap_st(df, 'Xloc', 'Yloc', feature, xmin, xmax, ymin, ymax, min, max, ('Location Map ' + str(feature)),'X (km)','Y (km)',\n",
        "             (str(feature) + ' (' + str(unit) + ')'), cmap)\n",
        "\n",
        "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.1, hspace=0.2); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRdZpq8o32gY"
      },
      "source": [
        "Maybe we can tighten up the color bar to see more details? and add some gridlines?\n",
        "\n",
        "* we craft our data and model plots to best communicate to our audience. It's fun to make great data and model plots!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lX_wLEeg32gY"
      },
      "outputs": [],
      "source": [
        "GSLIB.locmap_st(df,'Xloc', 'Yloc', feature, xmin, xmax, ymin, ymax,\n",
        "                0.7, 2.5, # set the value range for the color map\n",
        "                ('Location Map ' + str(feature)),'X (km)','Y (km)',\n",
        "             (str(feature) + ' (' + str(unit) + ')'),cmap); add_grid()\n",
        "\n",
        "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.1, hspace=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvLaWEeH32gY"
      },
      "outputs": [],
      "source": [
        "# We can also visualise the data from the validation dataset to a map\n",
        "GSLIB.locmap_st(df_2,'Xloc', 'Yloc', feature, xmin, xmax, ymin, ymax,\n",
        "                0.7, 2.5, # set the value range for the color map\n",
        "                ('Location Map ' + str(feature)),'X (km)','Y (km)',\n",
        "                (str(feature) + ' (' + str(unit) + ')'), cmap); add_grid()\n",
        "\n",
        "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.1, hspace=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu7J_uCS32gZ"
      },
      "source": [
        "The new colorbar extents improves the resolution of spatial details for our property.\n",
        "\n",
        "We will need ranges for the other variables. I'll pick some:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bp6RwYCq32gZ"
      },
      "outputs": [],
      "source": [
        "#Choose 4 features of interest\n",
        "\n",
        "feature1 = 'Cd'  # feature of interest\n",
        "feature2 = 'Zn'  # feature of interest\n",
        "feature3 = 'Pb'  # feature of interest\n",
        "feature4 = 'Cu'  # feature of interest\n",
        "\n",
        "# Set the range for the color map for each feature\n",
        "\n",
        "f1_min_cbar = np.floor(df[feature1].min()); f1_max_cbar = np.ceil(df[feature1].max())\n",
        "f2_min_cbar = np.floor(df[feature2].min()); f2_max_cbar = np.ceil(df[feature2].max())\n",
        "f3_min_cbar = np.floor(df[feature3].min()); f3_max_cbar = np.ceil(df[feature3].max())\n",
        "f4_min_cbar = np.floor(df[feature4].min()); f4_max_cbar = np.ceil(df[feature4].max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAB9eJPd32gZ"
      },
      "source": [
        "Let's add the other properties into a composite figure with all the plots as subplots.\n",
        "\n",
        "* To do this we use the subplot command, in matplotlib package to prior to the figure command to indicate that the following figure is part of a subplot and we use subplots_adjust at the end to get the scaling right.\n",
        "\n",
        "* We can save our fancy figure to an image file with the file format and resolution that we need.\n",
        "\n",
        "This is great for writing reports, papers and making great looking update presentations.\n",
        "\n",
        "Note, in GeostatsPy, I provide additional plotting methods with and without '_st' in the name.\n",
        "\n",
        "* with '_st' functions to 'stack' images in a composite figure.\n",
        "* without '_st' functions to produce a single image and simultaneously make a file\n",
        "\n",
        "We can also just make a file after we make our composite plot, see below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI-odLMw32gZ"
      },
      "outputs": [],
      "source": [
        "save_to_file = False                                          # save composite image to a file?\n",
        "\n",
        "plt.subplot(221)\n",
        "GSLIB.locmap_st(df, 'Xloc', 'Yloc', feature1, xmin, xmax, ymin, ymax, f1_min_cbar, f1_max_cbar,\n",
        "                'Location Map ' + str(feature1), 'X (km)', 'Y (km)', str(feature1) + ' (' + str(unit) + ')', cmap); add_grid()\n",
        "\n",
        "plt.subplot(222)\n",
        "GSLIB.locmap_st(df, 'Xloc', 'Yloc', feature2, xmin, xmax, ymin, ymax, f2_min_cbar, f2_max_cbar,\n",
        "                'Location Map ' + str(feature2), 'X (km)', 'Y (km)', str(feature2) + ' (' + str(unit) + ')', cmap); add_grid()\n",
        "\n",
        "plt.subplot(223)\n",
        "GSLIB.locmap_st(df, 'Xloc', 'Yloc', feature3, xmin, xmax, ymin, ymax, f3_min_cbar, f3_max_cbar,\n",
        "                'Location Map ' + str(feature3), 'X (km)', 'Y (km)', str(feature3) + ' (' + str(unit) + ')', cmap); add_grid()\n",
        "\n",
        "plt.subplot(224)\n",
        "GSLIB.locmap_st(df, 'Xloc', 'Yloc', feature4, xmin, xmax, ymin, ymax, f4_min_cbar, f4_max_cbar,\n",
        "                'Location Map ' + str(feature4), 'X (km)', 'Y (km)', str(feature4) + ' (' + str(unit) + ')', cmap); add_grid()\n",
        "\n",
        "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.2, hspace=0.2)\n",
        "if save_to_file == True:                                      # make a figure file\n",
        "    plt.savefig('All_location_maps.tif', dpi=600, bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8ZTmsmQ32gZ"
      },
      "source": [
        "Looks pretty good, eh? (yes, I am Canadian)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySsBJRho32gZ"
      },
      "source": [
        "Let's have a quick look at the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYAGGzZN32gZ"
      },
      "source": [
        "#### Visualizing Gridded Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0guFe0L32gZ"
      },
      "source": [
        "##### Loading Gridded Data\n",
        "\n",
        "Let's load and visualize a gridded, exhaustive data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzMCf46532gZ"
      },
      "source": [
        "Check the datatype of your gridded data.\n",
        "\n",
        "In this case it is actually also a .dat file, so we can use the same function to import it. The .grid extension was given to indicate that it is gridded data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "is-QGoru32gZ"
      },
      "outputs": [],
      "source": [
        "# define the path\n",
        "\n",
        "grid1_file_name = '//rocktype.grid'\n",
        "grid2_file_name = '//landuse.grid'\n",
        "\n",
        "# load the data\n",
        "\n",
        "df_grid = GSLIB.GSLIB2Dataframe(data_path + grid1_file_name)\n",
        "df_grid2 = GSLIB.GSLIB2Dataframe(data_path + grid2_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orKgO4T132gZ"
      },
      "outputs": [],
      "source": [
        "df_grid.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWNEG3MR32gZ"
      },
      "outputs": [],
      "source": [
        "# get the dimensions of the grid\n",
        "df_grid.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrnwiTl332gZ"
      },
      "outputs": [],
      "source": [
        "grid_feature = 'rocktype' # feature of interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxcaLs1632gZ"
      },
      "outputs": [],
      "source": [
        "# get the frequency of occurrence of each unique value in the grid\n",
        "df_grid[grid_feature].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nSlLPA32ga"
      },
      "source": [
        "##### Make Custom Colorbar\n",
        "\n",
        "We make this colorbar to display our categorical data, in this case rock type or land use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOd5DGPW32ga"
      },
      "outputs": [],
      "source": [
        "cmap_cat = plt.cm.get_cmap('Accent', 5) # make a colormap with 5 colors from Accent\n",
        "cmap_cat.set_over('white'); cmap_cat.set_under('white') # set the over and under value color"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o98MXfpl32ga"
      },
      "outputs": [],
      "source": [
        "GSLIB.locmap_st(df_grid,'x', 'y', grid_feature,\n",
        "                0, 5.2, ymin, ymax,\n",
        "                1, 5, # set the value range for the color map\n",
        "                ('Location Map ' + str(grid_feature)), 'X (km)', 'Y (km)',\n",
        "             (str(grid_feature) + ' ()' + str(unit) + ')'), cmap_cat)\n",
        "\n",
        "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.1, hspace=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rO7HQVsE32ga"
      },
      "source": [
        "##### Convert to ndarray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYZ1bbnW32ga"
      },
      "source": [
        "We have a cells in a grid but we need more information.\n",
        "\n",
        "* What is the origin, units, orientation and the cell size?\n",
        "\n",
        "This file format does not include that information so I'll give it to you.\n",
        "\n",
        "* cell size is 0.05 isotropic (same in x and y)\n",
        "\n",
        "We need to add the grid cell size, because we already have the grid extents (xmin, xmax, ymin and ymax) above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DF4le4DW32ga"
      },
      "outputs": [],
      "source": [
        "csize = 0.05                                                  # cell size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsgnLagH32ga"
      },
      "outputs": [],
      "source": [
        "# Round the x and y values to a consistent number of decimal places\n",
        "df_grid['x'] = df_grid['x'].round(2)\n",
        "df_grid['y'] = df_grid['y'].round(2)\n",
        "\n",
        "# Define the complete range of x and y values\n",
        "x_range = np.arange(df_grid['x'].min(), df_grid['x'].max() + csize, csize).round(2)\n",
        "y_range = np.arange(df_grid['y'].min(), df_grid['y'].max() + csize, csize).round(2)\n",
        "\n",
        "# Create a MultiIndex for the complete grid\n",
        "index = pd.MultiIndex.from_product([y_range, x_range], names=['y', 'x'])\n",
        "\n",
        "# Reindex the DataFrame to include all grid points\n",
        "df_complete = df_grid.set_index(['y', 'x']).reindex(index).reset_index()\n",
        "\n",
        "# Pivot the DataFrame to create a 2D array\n",
        "pivot_df = df_complete.pivot(index='y', columns='x', values='rocktype')\n",
        "\n",
        "# Reverse the order of the rows to match the Cartesian coordinate system\n",
        "pivot_df = pivot_df.iloc[::-1]\n",
        "\n",
        "# Convert the pivoted DataFrame to a 2D array and fill missing values with NaN\n",
        "array_2d = pivot_df.to_numpy()\n",
        "\n",
        "print(array_2d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjI0IszS32ga"
      },
      "outputs": [],
      "source": [
        "print('The object type is ' + str(type(array_2d)))\n",
        "print('  with size, ny =  ' + str(array_2d.shape[0]) + ' and nx = ' + str(array_2d.shape[1])) # check the grid size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QULfmYkz32ga"
      },
      "source": [
        "It is an NumPy ndarray - an array of values. Good!  \n",
        "\n",
        "Let's get more specific. We can use the 'type' command to find out what any object is and we can use the shape member of ndarray to get the size of the array (ny, nx)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCEdfZaA32ga"
      },
      "outputs": [],
      "source": [
        "type(array_2d)                                                 # check the type of the load object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNf1qOL732ga"
      },
      "source": [
        "Once again, no errors, a good sign. Let's see what we loaded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6PKtAb432ga"
      },
      "source": [
        "We will use the pixelplt command reimplemented form GSLIB. To see the parameters type the name and run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VQY0WZh32ga"
      },
      "outputs": [],
      "source": [
        "cmap_cat= plt.cm.get_cmap('Accent', 5) # make a colormap with 5 colors from Accent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEbnbE_v32ga"
      },
      "outputs": [],
      "source": [
        "array_2d.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfKhLEFB32ga"
      },
      "outputs": [],
      "source": [
        "df_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOcQI_Z932ga"
      },
      "outputs": [],
      "source": [
        "GSLIB.pixelplt_st(array_2d,\n",
        "                  df_grid['x'].min(),df_grid['x'].max(),df_grid['y'].min(),df_grid['y'].max(), #we have to use the actual min and max values\n",
        "                  csize,1,5,'Rock type','X (km)','Y (km)',\n",
        "                  (str(grid_feature)), cmap_cat); plt.show()\n",
        "\n",
        "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.2, hspace=0.2); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKdF6JRw32ga"
      },
      "source": [
        "Interesting, there are a lot of local variations in our subsurface unit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj0QvHOY32gb"
      },
      "source": [
        "#### Visualizing Tabular and Gridded Data Together\n",
        "\n",
        "You may be concerned about the consistency between the tabular samples and the gridded data.\n",
        "\n",
        "* It is a good check to plot them together.\n",
        "* GeostatsPy's locpix is a function to plot tabular and gridded data together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NolT99ph32gb"
      },
      "outputs": [],
      "source": [
        "GSLIB.locpix_st                                                  # GeostatsPy's combined tabular and gridded data plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXJ7BLWK32gb"
      },
      "outputs": [],
      "source": [
        "print(df.columns)\n",
        "cat_feature = 'Rock' # feature of interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkCGIU-x32gb"
      },
      "outputs": [],
      "source": [
        "GSLIB.locpix_st(array_2d\n",
        "                , df_grid['x'].min(), df_grid['x'].max(), df_grid['y'].min(), df_grid['y'].max(),\n",
        "                csize, 1, 5,\n",
        "                df, 'Xloc', 'Yloc',\n",
        "                cat_feature, 'Cd','X (km)','Y (km)',\n",
        "             str(cat_feature), cmap_cat); plt.show()\n",
        "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.2, hspace=0.2); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUncKapN32gb"
      },
      "source": [
        "What do we see?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_3K5BHF32gb"
      },
      "source": [
        "### Bivariate Analysis\n",
        "\n",
        "Michael J. Pyrcz, Professor, The University of Texas at Austin\n",
        "\n",
        "[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [Applied Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)\n",
        "\n",
        "Chapter of e-book \"Applied Geostatistics in Python: a Hands-on Guide with GeostatsPy\".\n",
        "\n",
        "Cite as: Pyrcz, M.J., 2024, Applied Geostatistics in Python: a Hands-on Guide with GeostatsPy, https://geostatsguy.github.io/GeostatsPyDemos_Book.\n",
        "\n",
        "By Michael J. Pyrcz <br />\n",
        "&copy; Copyright 2024."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dTtD98832gb"
      },
      "source": [
        "#### Bivariate Analysis\n",
        "\n",
        "This chapter is a tutorial for / demonstration of **Bivariate Analysis**.\n",
        "\n",
        "Let's start with simple bivariate scatter plots, and calculating bivariate statistics. Here's the scatter plots.\n",
        "\n",
        "* I included code below (commented out) for writing the plot as an image file. Note, you can write to many different formats, e.g., .tif, jpg, etc. and you can also control the resolution.\n",
        "\n",
        "* by saving in .pdf, .svg, or eps formats you create vector graphics that retain their sharpness at any resolution with economy of file size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erhhuAKG32gb"
      },
      "outputs": [],
      "source": [
        "# Define feature 1 and feature 2\n",
        "\n",
        "feature1 = 'Cd'; feature2 = 'Zn'                           # feature 1 and feature 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RMrdKiN32gb"
      },
      "outputs": [],
      "source": [
        "# if not defined yet;\n",
        "\n",
        "unit = 'ppm'                                                # unit of measurement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZg8Gx_r32gb"
      },
      "source": [
        "#### Visualizing Tabular Data with Location Maps\n",
        "\n",
        "Let's try out locmap. This is a reimplementation of GSLIB's locmap program that uses matplotlib. I hope you find it simpler than matplotlib, if you want to get more advanced and build custom plots lock at the source. If you improve it, send me the new code. Any help is appreciated. To see the parameters, just type the command name:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nju-cuj-32gb"
      },
      "source": [
        "The new colorbar extents improves the resolution of spatial details for our property.\n",
        "\n",
        "We will need ranges for the other variables. I'll pick some:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yxmj4MYJ32gb"
      },
      "outputs": [],
      "source": [
        "\n",
        "f1_min = df[feature1].min(); f1_max = df[feature1].max()    # range for feature 1\n",
        "f2_min = df[feature2].min(); f2_max = df[feature2].max()    # range for feature 2\n",
        "\n",
        "f1_min_cbar = np.floor(f1_min); f1_max_cbar = np.ceil(f1_max) # range for feature 1 color bar\n",
        "f2_min_cbar = np.floor(f2_min); f2_max_cbar = np.ceil(f2_max) # range for feature 2 color bar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkFUcIrV32gb"
      },
      "outputs": [],
      "source": [
        "# if not defined yet;\n",
        "\n",
        "xmin= 0; xmax = np.ceil(df.Xloc.max())                       # range of x values\n",
        "ymin= 0; ymax = np.ceil(df.Yloc.max())                       # range of y values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZCdg_dJ32gb"
      },
      "source": [
        "Let's add the other properties into a composite figure with all the plots as subplots.\n",
        "\n",
        "* To do this we use the subplot command, in matplotlib package to prior to the figure command to indicate that the following figure is part of a subplot and we use subplots_adjust at the end to get the scaling right.\n",
        "\n",
        "* We can save our fancy figure to an image file with the file format and resolution that we need.\n",
        "\n",
        "This is great for writing reports, papers and making great looking update presentations.\n",
        "\n",
        "Note, in GeostatsPy, I provide additional plotting methods with and without '_st' in the name.\n",
        "\n",
        "* with '_st' functions to 'stack' images in a composite figure.\n",
        "* without '_st' functions to produce a single image and simultaneously make a file\n",
        "\n",
        "We can also just make a file after we make our composite plot, see below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOzQqOFh32gb"
      },
      "outputs": [],
      "source": [
        "save_to_file = False                                          # save composite image to a file?\n",
        "\n",
        "plt.subplot(121)\n",
        "GSLIB.locmap_st(df,'Xloc','Yloc', feature1,\n",
        "                xmin, xmax, ymin, ymax, # as defined earlier\n",
        "                f1_min_cbar, f1_max_cbar, # adjust the color bar range\n",
        "                f'{feature1}',\n",
        "                'X (km)','Y (km)',\n",
        "                f'{feature1} ({unit})',\n",
        "                cmap\n",
        "                ); add_grid()\n",
        "\n",
        "plt.subplot(122)\n",
        "GSLIB.locmap_st(df,'Xloc','Yloc', feature2, xmin, xmax, ymin, ymax,\n",
        "                f2_min_cbar, f2_max_cbar,\n",
        "                f'{feature2}','X (km)','Y (km)',\n",
        "                f'{feature2} ({unit})', cmap); add_grid()\n",
        "\n",
        "# plt.subplot(223)\n",
        "# GSLIB.locmap_st(df,'Xloc','Yloc','Perm',xmin,xmax,ymin,ymax,permmin,permmax,'Well Data - Permeability','X(m)','Y(m)',\n",
        "#                 'Permeability (mD)',cmap); add_grid()\n",
        "\n",
        "# plt.subplot(224)\n",
        "# GSLIB.locmap_st(df,'Xloc','Yloc','AI',xmin,xmax,ymin,ymax,AImin,AImax,'Well Data - Acoustic Impedance','X(m)','Y(m)',\n",
        "#                 'Acoustic Impedance (kg/m2s*10^6)',cmap); add_grid()\n",
        "\n",
        "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.2, hspace=0.2)\n",
        "if save_to_file == True:                                      # make a figure file\n",
        "    plt.savefig('All_location_maps.tif',dpi=600,bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaX6fos232gb"
      },
      "source": [
        "Looks pretty good, eh? (yes, I am Canadian).\n",
        "\n",
        "\n",
        "#### Scatter plot\n",
        "\n",
        "Let's visualize the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iucPWaPe32gc"
      },
      "outputs": [],
      "source": [
        "plt.plot(df[feature1].values, df[feature2].values, 'o', label='', markerfacecolor='darkorange', markeredgecolor='black', alpha=0.8)\n",
        "plt.title(f'{feature1} vs. {feature2}')\n",
        "plt.xlabel(f'{feature1} ({unit})')\n",
        "plt.ylabel(f'{feature2} ({unit})')\n",
        "# plt.xlim([min, max])\n",
        "# plt.ylim([min, max])\n",
        "add_grid()\n",
        "# plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.8, wspace=0.3, hspace=0.2)\n",
        "#plt.savefig('Test.pdf', dpi=600, bbox_inches='tight', format='pdf')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jlxs2nom32gc"
      },
      "source": [
        "#### Correlation and Covariance\n",
        "\n",
        "It is straight forward to calculate the covariance and correlation from the pairs of data in our dataset. Here's the covariance matrix.  \n",
        "\n",
        "* Notice that the matrix is symmetrical? Makes sense, as the $C_{feature1,feature2} = C_{feature2,feature1}$.\n",
        "* Also, note that the diagonal values ($C_{i,j}$ where $i=j$) equal to the variance.\n",
        "\n",
        "We check the variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBIj4RSR32gc"
      },
      "outputs": [],
      "source": [
        "print(df.iloc[:, 4:].cov())                                   # the covariance matrix for columns 3,4,5 and 6 and all rows\n",
        "print(f'The variance of {feature} is ' + str(round(np.var(df[feature].values), 6)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQI1vE4t32gc"
      },
      "source": [
        "Here's the correlation coefficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWsYqRaB32gc"
      },
      "outputs": [],
      "source": [
        "df.iloc[:, 4:].corr()                                         # correlation matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ble6apwd32gc"
      },
      "source": [
        "#### Visualize the Correlation Matrix\n",
        "\n",
        "It is convenient to visualize the correlation matrix as follows.\n",
        "\n",
        "* I added a custom colour bar to communicate significance. Demonstration only, I did not calculate confidence intervals, but that could be added."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7G5OBi932gc"
      },
      "source": [
        "#### Declare Functions\n",
        "\n",
        "Let's define a single function to streamline plotting correlation matrices. I also added a convenience function to add major and minor gridlines to improve plot interpretability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DucSuN0Z32gc"
      },
      "outputs": [],
      "source": [
        "def plot_corr(corr_matrix,title,limits,mask):                 # plots a graphical correlation matrix\n",
        "    my_colormap = plt.cm.get_cmap('RdBu_r', 256)\n",
        "    newcolors = my_colormap(np.linspace(0, 1, 256))\n",
        "    white = np.array([256/256, 256/256, 256/256, 1])\n",
        "    white_low = int(128 - mask*128); white_high = int(128+mask*128)\n",
        "    newcolors[white_low:white_high, :] = white                # mask all correlations less than abs(0.8)\n",
        "    newcmp = ListedColormap(newcolors)\n",
        "    m = corr_matrix.shape[0]\n",
        "    im = plt.matshow(corr_matrix,fignum=0\n",
        "                    #  ,vmin = -1.0*limits, # for symmetrical distribution of values, adjust to 0 for positive only\n",
        "                     , vmin = 0,\n",
        "                     vmax = limits,cmap = newcmp)\n",
        "    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns)\n",
        "    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)\n",
        "    plt.colorbar(im, orientation = 'vertical')\n",
        "    plt.title(title)\n",
        "    for i in range(0,m):\n",
        "        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')\n",
        "        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')\n",
        "    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqtU1wn_32gc"
      },
      "outputs": [],
      "source": [
        "corr_matrix = df.iloc[:,4:].corr()\n",
        "plt.subplot(111)\n",
        "plot_corr(corr_matrix,'Correlation Matrix',1.0,0.5)           # using our correlation matrix visualization function\n",
        "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fh2Pycy32gc"
      },
      "source": [
        "To calculate a single correlation coefficient with the associate p-value for significance testing we can use SciPy's pearsonr function.\n",
        "\n",
        "* the p-value indicates the $\\alpha$ level at which we would reject the null hypothesis that the correlation coefficient is actually 0.0 and the value is due to random effect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dcj_225Q32gc"
      },
      "outputs": [],
      "source": [
        "corr, corr_p_value = pearsonr(df[feature1], df[feature2])\n",
        "\n",
        "plt.subplot(111)\n",
        "plt.plot(df[feature1].values,df[feature2].values, 'o', label='', markerfacecolor='darkorange', markeredgecolor='black', alpha=0.8)\n",
        "plt.title(f'{feature1} vs. {feature2}')\n",
        "plt.xlabel(f'{feature1} ({unit})')\n",
        "plt.ylabel(f'{feature2} ({unit})')\n",
        "# plt.xlim([min,max]); plt.ylim([min,max]);\n",
        "add_grid()\n",
        "\n",
        "plt.annotate(r'$\\rho_{\\phi,k}$ = ' + str(np.round(corr, 3)), xy=(0.1, 0.9), xycoords='axes fraction')\n",
        "plt.annotate(r'$\\rho_{\\phi,k}, \\alpha$ = ' + str(np.round(corr_p_value, 20)), xy=(0.1, 0.85), xycoords='axes fraction')\n",
        "\n",
        "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.2, wspace=0.3, hspace=0.2); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBAEBQf032gc"
      },
      "source": [
        "The alpha value for the correlation is very small, how small? We can check it like this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKpRO_rr32gc"
      },
      "outputs": [],
      "source": [
        "corr_p_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nRejgjs32gc"
      },
      "source": [
        "#### Spearman's Rank Correlation Coefficient\n",
        "\n",
        "The Spearman’s rank correlation coefficient, provides a measure of the degree of monotonic relationship.\n",
        "\n",
        "* Rank transform, e.g. $R_(x_i)$, sort the data in ascending order and replace the data with the index, $i=1,\\ldots,n$.\n",
        "* Spearman’s rank correlation coefficient is more robust in the presence of outliers and some nonlinear features than the Pearson’s correlation coefficient\n",
        "\n",
        "Let's try out the rank correlation coefficient with SciPy's stats module function.\n",
        "\n",
        "* it also provides a p-value for each measure for significance testing, i.e., if p-value < alpha then we reject the null hypothesis that the rank correlation coefficient is 0.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSoh0fjv32gc"
      },
      "outputs": [],
      "source": [
        "rank_correlation, rank_correlation_pval = stats.spearmanr(df.iloc[:,4:]) # calculate the range correlation coefficient\n",
        "rank_matrix = pd.DataFrame(rank_correlation,columns=corr_matrix.columns)\n",
        "print('Rank Correlation:')\n",
        "print(rank_correlation)\n",
        "print('\\nRank Correlation p-value:')\n",
        "print(rank_correlation_pval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP5Pk1KW32gc"
      },
      "source": [
        "#### Plot Correlation and Rank Correlation Matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05I2Fu_j32gd"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
        "\n",
        "# Plot correlation matrix\n",
        "im1 = axes[0].imshow(corr_matrix, cmap='Reds', vmin=0, vmax=1)\n",
        "axes[0].set_xticks(range(len(corr_matrix.columns)))\n",
        "axes[0].set_yticks(range(len(corr_matrix.columns)))\n",
        "axes[0].set_xticklabels(corr_matrix.columns, rotation=90)\n",
        "axes[0].set_yticklabels(corr_matrix.columns)\n",
        "axes[0].set_title('Correlation Matrix')\n",
        "\n",
        "# Plot rank correlation matrix\n",
        "im2 = axes[1].imshow(rank_matrix, cmap='Reds', vmin=0, vmax=1)\n",
        "axes[1].set_xticks(range(len(rank_matrix.columns)))\n",
        "axes[1].set_yticks(range(len(rank_matrix.columns)))\n",
        "axes[1].set_xticklabels(rank_matrix.columns, rotation=90)\n",
        "axes[1].set_yticklabels(rank_matrix.columns)\n",
        "axes[1].set_title('Rank Correlation Matrix')\n",
        "\n",
        "# Add a common colorbar\n",
        "cbar = fig.colorbar(im1, ax=axes, orientation='horizontal', fraction=0.046, pad=0.04)\n",
        "cbar.set_label('Correlation Coefficient')\n",
        "\n",
        "plt.subplots_adjust(left=0.05, bottom=0.2, right=0.95, top=0.9, wspace=0.3, hspace=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl6HAAnK32gd"
      },
      "source": [
        "Note that the y-axis order is plotted inversely"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4TIT6Gi32gd"
      },
      "outputs": [],
      "source": [
        "plt.subplot(221)                                              # plot correlation matrix with significance colormap\n",
        "plot_corr(corr_matrix,'Correlation Matrix',1.0,0.5)           # using our correlation matrix visualization function\n",
        "\n",
        "plt.subplot(222)                                              # plot correlation matrix with significance colormap\n",
        "plot_corr(rank_matrix,'Rank Correlation Matrix',1.0,0.5)      # using our correlation matrix visualization function\n",
        "\n",
        "plt.subplot(223)                                              # plot correlation matrix with significance colormap\n",
        "diff = corr_matrix.values - rank_matrix.values\n",
        "diff_matrix = pd.DataFrame(diff,columns=corr_matrix.columns)\n",
        "plot_corr(diff_matrix,'Correlation - Rank Correlation',0.3,0.0) # using our correlation matrix visualization function\n",
        "\n",
        "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.2, wspace=0.2, hspace=0.2); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_Gh17RV32gd"
      },
      "source": [
        "From the correlation minus rank correlation heatmap we see,\n",
        "\n",
        "*\n",
        "\n",
        "#### Matrix Scatter Plots\n",
        "\n",
        "If we have 3 or more variables to consider then matrix scatter plot offer an efficient method to display the multivariate relationships, 2 variables at a time. Once can identify:\n",
        "\n",
        "1. the range, envelope of the paired data\n",
        "2. homoscedastic and heteroscedastic behaviors\n",
        "3. nonlinear features\n",
        "\n",
        "Here's the seaborn package matrix scatter plot function, pairplot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1lj_0pz32gd"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(df, vars=df.columns[4:], markers='o',\n",
        "             corner=True, diag_kind='hist',\n",
        "            #  , diag_kws={'bins': 20} # If you don't specify the number of bins, it will use the Freedman-Diaconis rule which is a rule of thumb for determining the number of bins in a histogram\n",
        "            plot_kws={'alpha': 0.8, 'edgecolor': 'black', 'facecolor': 'darkorange'})\n",
        "\n",
        "# Add units to the labels, in this case they all have the same unit\n",
        "for ax in plt.gcf().axes:\n",
        "    ax.set_xlabel(ax.get_xlabel() + f' ({unit})')\n",
        "    ax.set_ylabel(ax.get_ylabel() + f' ({unit})')\n",
        "\n",
        "plt.subplots_adjust(left=0.0, bottom=0.0, right=0.9, top=0.9, wspace=0.2, hspace=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8YBjgYa32gd"
      },
      "source": [
        "We can also color our data by a categorical variable. If you have >2 categories, the histograms can get a bit messy, so we plot the as kernel density"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSs17G3I32gd"
      },
      "outputs": [],
      "source": [
        "pairplot = sns.pairplot(df,\n",
        "             hue='Rock', # color by column\n",
        "             vars=df.columns[4:], markers='o',\n",
        "             palette=sns.color_palette(cmap_cat.colors), # use the Accent color palette\n",
        "             corner=True,\n",
        "            #  diag_kind='hist'\n",
        "            #  diag_kws={'bins': 20}\n",
        "             )\n",
        "\n",
        "# Define the mapping for the legend\n",
        "legend_labels = {1.0: 'Argovian', 2.0: 'Kimmeridgian', 3.0: 'Sequanian', 4.0: 'Portlandian', 5.0: 'Quaternary'}\n",
        "# Adjust the legend\n",
        "for t, l in zip(pairplot._legend.texts, pairplot._legend.get_texts()):\n",
        "    t.set_text(legend_labels[float(l.get_text())])\n",
        "\n",
        "# Place the legend inside the figure\n",
        "pairplot._legend.set_bbox_to_anchor((0.5, 0.85))\n",
        "pairplot._legend.set_frame_on(True)\n",
        "pairplot._legend.set_title('Rock Types')\n",
        "\n",
        "plt.subplots_adjust(left=0.0, bottom=0.0, right=0.9, top=0.9, wspace=0.2, hspace=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LaHSMyP32gd"
      },
      "source": [
        "# Additional & alternative code sections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIRaCIG332gd"
      },
      "source": [
        "Please note these are optional code sections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crVuYAGR32gd"
      },
      "source": [
        "Here's some of the NumPy statistical functions that take ndarrays as an inputs.  With these methods if you had a multidimensional array you could calculate the average by row (axis = 1) or by column (axis = 0) or over the entire array (no axis specified). We just have a 1D ndarray so this is not applicable here.\n",
        "\n",
        "We calculate the inverse of the CDF, $F^{-1}_x(x)$ with Numpy percentile function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIxhhTTM32gd"
      },
      "outputs": [],
      "source": [
        "print('The minimum is ' + str(round(np.amin(df[feature].values),2))) # print univariate statistics\n",
        "print('The maximum is ' + str(round(np.amax(df[feature].values),2)))\n",
        "print('The range (maximum - minimum) is ' + str(round(np.ptp(df[feature].values),2)))\n",
        "print('The P10 is ' + str(round(np.percentile(df[feature].values,10),3)))\n",
        "print('The P50 is ' + str(round(np.percentile(df[feature].values,50),3)))\n",
        "print('The P90 is ' + str(round(np.percentile(df[feature].values,90),3)))\n",
        "print('The P13 is ' + str(round(np.percentile(df[feature].values,13),3)))\n",
        "print('The median (P50) is ' + str(round(np.median(df[feature].values),3)))\n",
        "print('The mean is ' + str(round(np.mean(df[feature].values),3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYokEQ3t32gd"
      },
      "source": [
        "We can calculate the CDF value, $F_x(x)$, directly from the data.\n",
        "* we apply a conditional statement to our ndarray to calculate a boolean ndarray with the same size of the data and then count the cases that meet the condition\n",
        "* note, we are assuming equal weighting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhvS-P-y32gd"
      },
      "outputs": [],
      "source": [
        "value = 0.10                                                  # calculate cumulative distribution function for a specified value\n",
        "cumul_prob = np.count_nonzero(df[feature].values <= value)/len(df)\n",
        "print('The cumulative probability for ' + str(feature) + ' = ' + str(value) + ' is ' + str(round(cumul_prob,2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tspQlr6832gd"
      },
      "source": [
        "#### Plotting a Histogram with the matplotlib Package\n",
        "\n",
        "I don't want to suggest that matplotlib is hard to use. The GSLIB visualizations provide convenience and once again use the same parameters as the GSLIB methods. Particularly, the 'hist' function is pretty easy to use, just a lot more code to write.  It doens't have default values for the arguments.\n",
        "\n",
        "* here's how we can make the same histogram as above with matplotlib directly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2SW0P_032gd"
      },
      "outputs": [],
      "source": [
        "plt.hist(df[feature].values, alpha=0.8, color=\"darkorange\", edgecolor=\"black\", bins=20, range=[min,max])\n",
        "plt.title('Histogram'); plt.xlabel(str(feature) + ' (' + str(unit) + ')'); plt.ylabel(\"Frequency\"); add_grid()\n",
        "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.1, hspace=0.2); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPN8VYks32ge"
      },
      "source": [
        "#### Plotting a CDF with the Matplotlib Package\n",
        "\n",
        "Here's how we calculate and plot a CDF with matplotlib.  \n",
        "* the y axis is cumulative probability with a minimum of 0.0 and maximum of 1.0 as expected for a CDF.\n",
        "* note after the initial hist command we can add a variety of elements such as labels to our plot as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGD4qyK632ge"
      },
      "outputs": [],
      "source": [
        "print(df[feature].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzBs9kWn32ge"
      },
      "outputs": [],
      "source": [
        "plt.hist(df[feature].values,density=True, cumulative=True, label='CDF',\n",
        "           histtype='stepfilled', alpha=0.8, bins = 100, color='darkorange', edgecolor = 'black', range=[min, max])\n",
        "plt.xlabel(str(feature) + ' (' + str(unit) + ')'); plt.title(str(feature) + ' CDF'); plt.ylabel('Cumulation Probability'); add_grid()\n",
        "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.1, hspace=0.2); plt.xlim([min,max]); plt.ylim([0,1])\n",
        "#plt.savefig('cdf_' + str(feature) + .tif',dpi=600,bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vw4ixTEt32ge"
      },
      "source": [
        "# Additional text and YouTube links from Michael Pyrcz along with the code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAgh80NA32ge"
      },
      "source": [
        "**YouTube Lecture**: check out my lectures on:\n",
        "\n",
        "* [Univariate Distributions](https://youtu.be/TbqaMXdSV4I?si=tzPNssh5Qcqv6DY_)\n",
        "* [Statistical Expectation](https://youtu.be/QVgMt3cPMmM?si=otR-qEN8xeEm2nbk)\n",
        "* [Parametric Distributions](https://youtu.be/U7fGsqCLPHU?si=ekNqbNqUvrGdTjjG)\n",
        "* [Joint, Marginal, and Conditional Probability](https://youtu.be/kxjnPVyuuo8?si=FI3Tiu72Wc5Neunm)\n",
        "\n",
        "For convenience here's a summary of the salient points.\n",
        "\n",
        "#### Definitions\n",
        "\n",
        "Let's start with some basic definitions with respect to univariate, bivariate and multivariate.\n",
        "\n",
        "* **Univariate** - involving one variable (feature) or event.\n",
        "\n",
        "* **Univariate Statistics** - summary measures based on one feature measured over a sample\n",
        "\n",
        "* **Univariate Parameters** - summary measures inferred for one feature measured over the population\n",
        "\n",
        "We start with univariate, but we will cover bivariate, involving two variables (features) later. Note, joint probabilities and distributions are:\n",
        "\n",
        "* **Bivariate** - regarding 2 variables (features)\n",
        "\n",
        "* **Multivariate** - the general term for $> 1$ features, but often refers to $\\ge 3$ or more).\n",
        "\n",
        "* **Massively Multivariate** - high dimensional, usually indicating 7 or more features\n",
        "\n",
        "Now, let's describe the concept of a distribution.\n",
        "\n",
        "* **Statistical Distribution** – for a variable (feature) a description of the probability of occurrence over the range of possible values. What do we get from a statistical distribution?\n",
        "\n",
        "    * what is the minimum and maximum?\n",
        "\n",
        "    * do we have a lot of low values?\n",
        "\n",
        "    * do we have a lot of high values?\n",
        "\n",
        "    * do we have outliers (values that don’t make sense and need an explanation)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj-jkPgK32ge"
      },
      "source": [
        "**YouTube Lecture**: check out my lecture on [Q-Q and P-P Plots](https://youtu.be/wCgdoImlLY0?si=lpTWz2H7QTdxHBy9). For your convenience here's a summary of salient points.\n",
        "\n",
        "##### QQ-Plot\n",
        "\n",
        "The scatter plot of matching percentiles between two distribution. Why learn about QQ-Plots?\n",
        "\n",
        "* convenient plot to compare distributions for 2 features\n",
        "\n",
        "* a function fit to a QQ-plot is the distribution transform, forward,\n",
        "\n",
        "$$\n",
        "y = F_y^{-1}\\left( F_x(x) \\right)\n",
        "$$\n",
        "\n",
        "and reverse,\n",
        "\n",
        "$$\n",
        "x = F_x^{-1}\\left( F_y(y) \\right)\n",
        "$$\n",
        "\n",
        "##### QQ-Plot Interpretation\n",
        "\n",
        "If the two distributions are the same, then all the percentiles will be equal and the points will all fall on the 45 degree line. Here's an example with very similar distributions.\n",
        "\n",
        "If the means of the two distributions are different, then the points will be shifted from the 45 degree line.\n",
        "\n",
        "* down and right from the 45 degree line if distribution on x-axis is has a larger mean than the distribution on the y-axis\n",
        "\n",
        "* up and left from the 45 degree line if distribution on x-axis is has a smaller mean than the distribution on the y-axis\n",
        "\n",
        "If the variances (or standard deviations) of the two distributions are different, then the points will appear to be stretched out along the axis for the distribution with greater variance.\n",
        "\n",
        "* difference in variance will appear like a \"rotation\" from the 45 degree line\n",
        "\n",
        "Of course, both the mean and variance can be different.\n",
        "\n",
        "Finally, it is possible for distributions to be similar and then to diverge only for part of the distribution. This will be quite clear on a QQ-plot.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz6RXsxp32ge"
      },
      "source": [
        "\n",
        "**YouTube Lecture**: check out my lecture on [Displaying Data](https://youtu.be/TbqaMXdSV4I?si=UwGH-iRc7V962yNF), see the first part of this larger lecture on univariate distributions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8Odnl7132ge"
      },
      "source": [
        "**YouTube Lecture**: check out my lectures on:\n",
        "\n",
        "* [Bivariate Statistics](https://youtu.be/wZwYEDqB4A4?si=yZLaxiGMv50K841R)\n",
        "* [Multivariate Analysis](https://youtu.be/Ui2El5CZPRE?si=uA_IqurENzC6Owbg)\n",
        "\n",
        "These lectures are all part of my [Data Analytics and Geostatistics Course](https://www.youtube.com/playlist?list=PLG19vXLQHvSB-D4XKYieEku9GQMQyAzjJ) on YouTube with linked well-documented Python workflows and interactive dashboards. My goal is to share accessible, actionable, and repeatable educational content. If you want to know about my motivation, check out [Michael's Story](https://michaelpyrcz.com/my-story).\n",
        "\n",
        "#### Motivation for Bivariate Analysis\n",
        "\n",
        "There are many important reasons to learn bivariate analysis methods to strengthen your foundation in geostatistics,\n",
        "\n",
        "**Never a Single Feature** - most often we work with multiple features. It is quite rare to model a single feature as part of a geostatistical modeling workflow.\n",
        "\n",
        "* petroleum rock and fluid system features - facies, porosity, permeability and fluid saturations\n",
        "\n",
        "* mining deposit features - concentration of copper, nickel, silver, gold and sulfur\n",
        "\n",
        "* agriculture soil features - pH level, soil moisture, nitrogen, phosphorus, clay content and organic matter\n",
        "\n",
        "* forestry inventory features - tree density, species composition, tree height, basal area and above-ground biomass\n",
        "\n",
        "Our spatial models are multivariate, so we cannot model a single feature in isolation.\n",
        "\n",
        "**Geostatistics is Limited to Multivariate** - I do not like making this confession. While some will respond with a comment about some novel workflow that builds multivariate geostatistical models, for example [Step-wise Conditional Transform](https://link.springer.com/article/10.1023/A:1023235505120) by Dr. Leuangthong and Professor Deutsch.\n",
        "\n",
        "* yet, the common practice is limited to 2 features at a time, primary feature being modeled supported by a secondary feature\n",
        "\n",
        "* this is based on variants of the cokriging system that we introduce in the [Cosimulation](GeostatsPy_cosimulation) chapter\n",
        "\n",
        "* a sequential approach is applied model more than 2 features, for example, simulate copper, cosimulate nickel with copper as secondary, cosimulate gold with nickel as secondary\n",
        "\n",
        "To do this we must understand bivariate analysis methods.\n",
        "\n",
        "#### Bivariate Analysis\n",
        "\n",
        "Understand and quantify the relationship between two variables, i.e., the bivariate probability density function, $f_{x,y}(x,y)$, for example,\n",
        "\n",
        "* porosity and permeability\n",
        "\n",
        "* fraction of shale and oil recovery\n",
        "\n",
        "* copper and gold\n",
        "\n",
        "* basal area and tree height\n",
        "  \n",
        "How can we use these relationships? To improve our prediction models and ultimately make better decisions.\n",
        "\n",
        "What would be the impact if we ignore these relationships and simply modeled each feature separately?\n",
        "\n",
        "* **assumption of independence** - there will be no relationship between our estimates or simulated realizations, the joint probability density function will not be reproduced and unrealistic combinations may occur, e.g., very high permeability with very low porosity\n",
        "\n",
        "* **data conditioning** - will impose the relationship at data locations. The geostatistics models honor the data at the data locations and the data scatter plot is the correct relationship, but away from the data this relationship will fade\n",
        "\n",
        "\n",
        "Bivariate statistics are our tools to quantify and impose these bivariate relationships in our geostatistical models.\n",
        "\n",
        "#### Bivariate Statistics\n",
        "\n",
        "**Pearson’s Product‐Moment Correlation Coefficient** or just the correlation coefficient or just correlation, is a widely-known bivariate statistic, if you say 0.9 correlation most engineers and scientists can visualize this! Here's the important points and limitations for the correlation coefficient,\n",
        "\n",
        "* the notation for correlation is, $\\rho_{x,y}$\n",
        "\n",
        "* provides a measure of the degree of linear relationship, i.e., nonlinear relationships will bias the correlation coefficient\n",
        "\n",
        "* is very sensitive to outliers, i.e., a single extreme outlier can significantly increase or reduce the apparent correlation\n",
        "\n",
        "There are some other deeper points that should be made about the Gaussian assumption,\n",
        "\n",
        "* the correlation coefficient provides a complete description for the bivariate relationship only under the assumption of bivariate Gaussianity. If we assume Gaussian, then the means, $\\overline{x}$ and $\\overline{y}$, standard deviations, $s_x$ and $s_y$, with the correlation coefficient, $\\rho_{x,y}$ completely describe the bivariate probability density function $f_{x,y}(x,y)$\n",
        "\n",
        "* for any other distributions, the correlation coefficient is not sufficient to know the entire, bivariate probability density function $f_{x,y}(x,y)$, e.g., to calculate conditional distributions, $f_{x|y}$\n",
        "\n",
        "* the standard approach in geostatistics is to apply Gaussian anamorphosis to each feature and then to assume bivariate Gaussian so that the correlation coefficient is sufficient to describe the relationship between the two features\n",
        "\n",
        "What about spatial bivariate relationship?\n",
        "\n",
        "* the full cokriging system includes cross variograms and cross covariance that capture the relationships between two features over space! But due to the difficulty of inferring and modeling the required linear model of coregionalization, this is rarely done.\n",
        "\n",
        "* instead a simplifying apply like collocated cokriging is applied that only retains the collocated secondary data and limits the model of bivariate relationship to the collocated (lag $\\bf{h}$ of 0.0) correlation coefficient\n",
        "\n",
        "More could be done! Let's review the sample variance of variable $x$. Of course, I'm truncating our notation as $x$ is a set of samples a locations in our modeling space, $x(\\bf{u_\\alpha}), \\, \\forall \\, \\alpha = 0, 1, \\dots, n - 1$.\n",
        "\n",
        "$$\n",
        "\\sigma^2_{x}  = \\frac{\\sum_{i=1}^{n} (x_i - \\overline{x})^2}{(n-1)}\n",
        "$$\n",
        "\n",
        "We can expand the squared term and replace on of them with $y$, another variable in addition to $x$.\n",
        "\n",
        "$$\n",
        "C_{xy}  = \\frac{\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})}{(n-1)}\n",
        "$$\n",
        "\n",
        "We now have a measure that represents the manner in which variables $x$ and $y$ co-vary or vary together.  We can standardized the covariance by the product of the standard deviations of $x$ and $y$ to calculate the correlation coefficient.\n",
        "\n",
        "$$\n",
        "\\rho_{xy}  = \\frac{\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})}{(n-1)\\sigma_x \\sigma_y}, \\, -1.0 \\le \\rho_{xy} \\le 1.0\n",
        "$$\n",
        "\n",
        "In summary we can state that the correlation coefficient is related to the covariance as:\n",
        "\n",
        "$$\n",
        "\\rho_{xy}  = \\frac{C_{xy}}{\\sigma_x \\sigma_y}\n",
        "$$\n",
        "\n",
        "To help you calibrate you eye to the correlation coefficient I developed an interactive [correlation dashboard](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Correlation_Coefficient.ipynb).\n",
        "\n",
        "* for the bivariate Gaussian case vary the correlation coefficient and observe the samples, condition expectation and joint probability density\n",
        "\n",
        "The Person's correlation coefficient is quite sensitive to outliers and departure from linear behavior (in the bivariate sense).\n",
        "\n",
        "For an more robust correlation measure in the presence of outliers and nonlinearity, we have an alternative known as the Spearman's rank correlations coefficient.   \n",
        "\n",
        "$$\n",
        "\\rho_{R_x R_y}  = \\frac{\\sum_{i=1}^{n} (R_{x_i} - \\overline{R_x})(R_{y_i} - \\overline{R_y})}{(n-1)\\sigma_{R_x} \\sigma_{R_y}}, \\, -1.0 \\le \\rho_{R_x,R_y} \\le 1.0\n",
        "$$\n",
        "\n",
        "The rank correlation applies the rank transform to the data prior to calculating the correlation coefficient.  To calculate the rank transform simply replace the data values with the rank $R_x = 1,\\dots,n$, where $n$ is the maximum value and $1$ is the minimum value.\n",
        "\n",
        "$$\n",
        "x_\\alpha, \\, \\forall \\alpha = 1,\\dots, n, \\, | \\, x_i \\ge x_j \\, \\forall \\, i \\gt j\n",
        "$$\n",
        "\n",
        "$$\n",
        "R_{x_i} = i\n",
        "$$\n",
        "\n",
        "To compare Pearson and rank correlation in the presence of an outlier I developed this [correlation outlier dashboard](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Correlation_Coefficient_Issues.ipynb),\n",
        "\n",
        "The correlation coefficients provide useful metrics to quantify relationships between two variables at a time. We can also consider bivariate scatter plots and matrix scatter plots to visualize multivariate data.\n",
        "\n",
        "* remember, current practical subsurface modeling is bivariate, two variables at a time    \n",
        "\n",
        "#### Multivariate Statistics\n",
        "\n",
        "While we are limited to bivariate models, we need to understand some multivariate methods to work with multivariate data.\n",
        "\n",
        "* only a short summary is provided here, see the discussion on joint and conditional probabilities and distributions in the [Probability Concepts](GeostatsPy_probability) chapter\n",
        "\n",
        "* see my lecture on [Multivariate Analysis](https://youtu.be/Ui2El5CZPRE?si=uA_IqurENzC6Owbg)\n",
        "\n",
        "Here's a short summary of multivariate statistics. If we define a probability density function (PDF) of feature $X_1$ as,\n",
        "\n",
        "$$\n",
        "f_{X_1}(x_1)\n",
        "$$\n",
        "\n",
        "we can extend the definition to a joint PDF for any arbitrary number, e.g., $m$, features as,\n",
        "\n",
        "$$\n",
        "f_{X_1,\\ldots,X_m}(x_1,\\ldots,x_m)\n",
        "$$\n",
        "\n",
        "Similarly, conditional PFDs can be calculated,\n",
        "\n",
        "$$\n",
        "f_{Y | X}(y | x)\n",
        "$$\n",
        "\n",
        "and extended to any number of features, e.g., $m$, features as,\n",
        "\n",
        "$$\n",
        "f_{Y | X_1, \\ldots, X_m}(y | x_1,\\ldots,x_m)\n",
        "$$\n",
        "\n",
        "and many of our statistical concepts extend to high dimensions, for example consider the correlation coefficient,\n",
        "\n",
        "$$\n",
        "\\rho_{X,Y}\n",
        "$$\n",
        "\n",
        "between 2 features and now the partial correlation coefficient that calculates correlation between 2 features ($X$ and $Y$) while controlling for all other features ($Z$).\n",
        "\n",
        "$$\n",
        "\\rho_{X,Y | Z}\n",
        "$$\n",
        "\n",
        "Ultimately the difficulty with calculating high dimensionality (massively multivariate) probabilities and statistics is inferential, we rarely have enough data to observe vast multivariate space and to model probabilities and statistics in that space!\n",
        "\n",
        "* See my linked lecture on the curse of dimensionality above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP7L2dHj32ge"
      },
      "source": [
        "# Comments\n",
        "\n",
        "This was a basic demonstration of calculating univariate statistics and visualizing data distributions. Much more could be done, I have other demonstrations on basics of working with DataFrames, ndarrays and many other workflows available at https://github.com/GeostatsGuy/PythonNumericalDemos and https://github.com/GeostatsGuy/GeostatsPy.\n",
        "\n",
        "I hope this was helpful,\n",
        "\n",
        "*Michael*\n",
        "\n",
        "#### The Author:\n",
        "\n",
        "Michael Pyrcz, Professor, The University of Texas at Austin\n",
        "*Novel Data Analytics, Geostatistics and Machine Learning Subsurface Solutions*\n",
        "\n",
        "With over 17 years of experience in subsurface consulting, research and development, Michael has returned to academia driven by his passion for teaching and enthusiasm for enhancing engineers' and geoscientists' impact in subsurface resource development.\n",
        "\n",
        "For more about Michael check out these links:\n",
        "\n",
        "[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [Applied Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)\n",
        "\n",
        "#### Want to Work Together?\n",
        "\n",
        "I hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.\n",
        "\n",
        "* Want to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? I'd be happy to drop by and work with you!\n",
        "\n",
        "* Interested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PIs including Profs. Foster, Torres-Verdin and van Oort)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!\n",
        "\n",
        "* I can be reached at mpyrcz@austin.utexas.edu.\n",
        "\n",
        "I'm always happy to discuss,\n",
        "\n",
        "*Michael*\n",
        "\n",
        "Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The Jackson School of Geosciences, The University of Texas at Austin\n",
        "\n",
        "More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [Applied Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "geostatspy",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}